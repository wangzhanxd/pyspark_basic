{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_*_coding:utf-8_*_\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.context import SparkConf\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"test\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "###获取路径下所以文件 形成RDD\n",
    "rdd = sc.wholeTextFiles(\"./data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('file:/home/admp/wangzhan/jupyter/pyspark/data/wc1.txt',\n",
       "  'hello world\\nhello Ireland\\nhello ML\\nhello Neo4j'),\n",
       " ('file:/home/admp/wangzhan/jupyter/pyspark/data/wc2.txt',\n",
       "  'hello google\\nhello facebook\\nhello XD\\nhello UCAS')]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('file:/home/admp/wangzhan/jupyter/pyspark/data/wc1.txt',\n",
       "  'hello world\\nhello Ireland\\nhello ML\\nhello Neo4j')]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello world', 'hello spark', 'hello hive', 'hello hadoop']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 文本总读取RDD\n",
    "rdd = sc.textFile(\"wc.txt\")\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello world', 'hello spark', 'hello hive', 'hello hadoop']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## list创建RDD\n",
    "rdd = sc.parallelize(['hello world', 'hello spark', 'hello hive', 'hello hadoop'])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word count example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world', 'hello', 'spark', 'hello', 'hive', 'hello', 'hadoop']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 单词分割\n",
    "rdd0 = rdd.flatMap(lambda line:line.split(\" \"))\n",
    "rdd0.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 1),\n",
       " ('world', 1),\n",
       " ('hello', 1),\n",
       " ('spark', 1),\n",
       " ('hello', 1),\n",
       " ('hive', 1),\n",
       " ('hello', 1),\n",
       " ('hadoop', 1)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## map\n",
    "rdd1 = rdd0.map(lambda x:(x,1))\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 4), ('world', 1), ('spark', 1), ('hive', 1), ('hadoop', 1)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##recude\n",
    "rdd2 = rdd1.reduceByKey(lambda a,b:a+b)\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 查看分区数\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello world', 'hello spark', 'hello hive', 'hello hadoop']]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello world', 'hello spark', 'hello hive', 'hello hadoop'], []]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 重新分区\n",
    "rdd.repartition(2).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello world', 'hello spark', 'hello hive', 'hello hadoop']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 数据按照分区形式打印 \n",
    "## 如果是四个分区\n",
    "## 可能是[['hello world'], ['hello spark'], ['hello hive'], ['hello hadoop']]\n",
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## flatMap 将map方法的结果 flap\n",
    "rdd = sc.parallelize([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[range(0, 1), range(0, 2), range(0, 3), range(0, 4)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd0 = rdd.map(lambda x:range(x))\n",
    "rdd0.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 1, 2, 0, 1, 2, 3]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = rdd.flatMap(lambda x:range(x))\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 聚合类 reduce fold/foldLeft/foldRight aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3,4])\n",
    "def add(a,b):\n",
    "    return a+b\n",
    "rdd.reduce(lambda a,b:a+b) ##rdd.reduce(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.fold(3,lambda a,b:a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 4], [6, 1]]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 分区聚合 初始化两个分区\n",
    "rdd =sc.parallelize([2,4,6,1],2)\n",
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 每个分区聚合 用seqOp，分区之间聚合用combOp\n",
    "zeroValue = 0\n",
    "seqOp = lambda a,b:a+b\n",
    "combOp = lambda x,y:x+y\n",
    "rdd.aggregate(zeroValue,seqOp,combOp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 2, 3, 4, 3, 4, 5, 4, 5, 6, 5, 6, 7]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5])\n",
    "rdd1 = rdd.flatMap(lambda x:(x,x+1,x+2))\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 4, 4, 4, 6, 6]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.filter(lambda x:x%2==0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交集intersection 并集union 排序sortBy 笛卡尔积cartesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd0 = sc.parallelize([\"B\",\"B\",\"A\"])\n",
    "rdd1 = sc.parallelize([\"B\",\"C\"])\n",
    "rdd0.intersection(rdd1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B', 'B', 'A', 'B', 'C']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd0.union(rdd1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'B', 'B']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd0.sortBy(lambda x:x,ascending = True).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', 'B'), ('B', 'C'), ('B', 'B'), ('B', 'C'), ('A', 'B'), ('A', 'C')]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd0.cartesian(rdd1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PairRdd的操作 groupByKey reduceByKey aggregateByKey reduceByKeyLocally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([\"zhangsan\",\"lisi\",\"wangwu\",\"zhangsan\",\"lisi\",\"wangwu\",\"zhangsan\",\"lisi\",\"wangwu\",\"zhangsan\",\"lisi\",\"wangwu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd0 = rdd.map(lambda x:(x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('zhangsan', 1),\n",
       " ('lisi', 1),\n",
       " ('wangwu', 1),\n",
       " ('zhangsan', 1),\n",
       " ('lisi', 1),\n",
       " ('wangwu', 1),\n",
       " ('zhangsan', 1),\n",
       " ('lisi', 1),\n",
       " ('wangwu', 1),\n",
       " ('zhangsan', 1),\n",
       " ('lisi', 1),\n",
       " ('wangwu', 1)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd0.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('zhangsan', <pyspark.resultiterable.ResultIterable at 0x7f064c150cc0>),\n",
       " ('lisi', <pyspark.resultiterable.ResultIterable at 0x7f064c150cf8>),\n",
       " ('wangwu', <pyspark.resultiterable.ResultIterable at 0x7f064c5e39b0>)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd0.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('zhangsan', 4), ('lisi', 4), ('wangwu', 4)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd0.reduceByKey(lambda a,b:a+b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroValue = 0\n",
    "sqlFunc = lambda a,b:a+b\n",
    "combFunc = lambda a,b:a+b\n",
    "rdd3 = rdd0.aggregateByKey(zeroValue,sqlFunc,combFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('zhangsan', 4), ('lisi', 4), ('wangwu', 4)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'zhangsan': 4, 'lisi': 4, 'wangwu': 4}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## reduceByKeyLocally 返回字典\n",
    "rdd0.reduceByKeyLocally(lambda a,b:a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算子分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD Transformation 算子 \n",
    "map filter distinct sample sortBy groupByKey sortByKey reduceByKey aggregateByKey sampleByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 采样\n",
    "rdd = sc.parallelize([1,2,3,2,3,4,2,2,3,4,5,3])\n",
    "rdd.sample(withReplacement =True,fraction = 0.2).collect()  ## withReplacement 是否有放回采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 连写 比较长的python代码可以用（） 然后直接换行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([\"Hello world\",\"hello hadoop\",\"hadoop Spark\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 2), ('world', 1), ('hadoop', 2), ('spark', 1)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRdd = (rdd.\n",
    "          flatMap(lambda x:x.split(\" \")).\n",
    "          map(lambda x:x.lower()).  ## 大小写转换\n",
    "          map(lambda x:(x,1)).\n",
    "          reduceByKey(lambda a,b:a+b))\n",
    "pairRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('world', 1)]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## sampleByKey 默认泊松抽样 可以指定伯努利\n",
    "rdd = sc.parallelize([\"hello world\",\"hello world\",\"hello world\"])\n",
    "(rdd.\n",
    "flatMap(lambda x:x.split(\" \")).\n",
    "map(lambda x:x.lower()).  ## 大小写转换\n",
    "map(lambda x:(x,1)).\n",
    "sampleByKey(withReplacement =True,fractions = {\"hello\":0.2,\"world\":0.2})).collect()  ### fractions 传入每个key的fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "### join\n",
    "# join leftOuterjoin rightOuterJoin fullOuterJoin\n",
    "rdd1 = sc.parallelize([('cat', 2), ('cat', 5), ('book', 4), ('cat', 12)])\n",
    "rdd2 = sc.parallelize([('cat', 2), ('cup', 5), ('mouse', 4), ('cat', 12)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', (2, 2)),\n",
       " ('cat', (2, 12)),\n",
       " ('cat', (5, 2)),\n",
       " ('cat', (5, 12)),\n",
       " ('cat', (12, 2)),\n",
       " ('cat', (12, 12))]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.join(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method join in module pyspark.rdd:\n",
      "\n",
      "join(other, numPartitions=None) method of pyspark.rdd.RDD instance\n",
      "    Return an RDD containing all pairs of elements with matching keys in\n",
      "    C{self} and C{other}.\n",
      "    \n",
      "    Each pair of elements will be returned as a (k, (v1, v2)) tuple, where\n",
      "    (k, v1) is in C{self} and (k, v2) is in C{other}.\n",
      "    \n",
      "    Performs a hash join across the cluster.\n",
      "    \n",
      "    >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      "    >>> y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
      "    >>> sorted(x.join(y).collect())\n",
      "    [('a', (1, 2)), ('a', (1, 3))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(rdd1.join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action 算子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### excutor返回driver的API\n",
    "#collect() take() first()  生产不用collect()，可能导致driver崩溃\n",
    "\n",
    "### 输出\n",
    "#foreach foreachPartition\n",
    "\n",
    "###保存文件到外部存储系统\n",
    "#saveAsTextFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rdd 数据输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd.foreachPartition 可以连接MySQL Hbase,写入数据库和数据仓库\n",
    "#dstream.foreachRdd\n",
    "## saveAsTextFile  保存成textFile\n",
    "## saveAsHadoopDataSet(jobconf())\n",
    "## TableoutputFormat\n",
    "## saveAsHadoopFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "### topN\n",
    "import random\n",
    "rdd = sc.parallelize([('cat', 2), ('cat', 5), ('book', 6), ('cat', 12),('book', 8),('book', 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 12), ('book', 8), ('book', 6)]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.top(3,key = lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "### topN 方法一 两阶段聚合（key +  随机前缀 + 局部聚合 + 全局聚合）\n",
    "### 防止数据倾斜 添加随机前缀\n",
    "### 局部去top2\n",
    "### 局部的top2里面求全局top2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topn(key,iter):\n",
    "    sortedIter = sorted(iter,reverse = True)\n",
    "    top2 = sortedIter[:2]\n",
    "    return map(lambda x:(key,x),top2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = (rdd.mapPartitions(lambda iter:map(lambda x:((random.randint(1,10),x[0]),x[1]),iter))\n",
    "         .groupByKey()\n",
    "         .flatMap(lambda x:topn(x[0][1],x[1]))\n",
    "         .groupByKey()\n",
    "         .flatMap(lambda x:topn(x[0],x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 2), ('cat', 5), ('book', 6), ('cat', 12), ('book', 8), ('book', 4)]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 方式2 使用aggregatrBykey  两阶段聚合函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', [12, 5]), ('book', [8, 6])]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(a,b):\n",
    "    a.append(b)\n",
    "    sortedIter = sorted(a,reverse = True)\n",
    "    top2 = sortedIter[:2]\n",
    "    return top2\n",
    "def h(x,y):\n",
    "    sortedIter = sorted(x+y,reverse = True)\n",
    "    top2 = sortedIter[:2]\n",
    "    return top2\n",
    "rdd.aggregateByKey(zeroValue = [],seqFunc = lambda a,b:f(a,b),combFunc = lambda x,y:h(x,y)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
